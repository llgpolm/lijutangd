{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Neural Network Design Choices Affect Loss Landscapes\n",
    "In this notebook, we explore the appearance of the loss landscapes of neural networks on the MNIST image classification task, under a number of different transformations. We will be using the `loss-landscapes` package to compute low-dimensional approximations of the loss function. We will be implementing the networks in PyTorch, which is the only supported neural network library as of March 2019 (more will be added later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add project source to path for use in the notebook\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is required to add the module source to Python's `path` variable, so that this notebook can import the `loss-landscapes` package. It is not required in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [20, 8]\n",
    "\n",
    "# code from this library - import the lines module\n",
    "import loss_landscapes.compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before we begin, we set some hyperparameters as constants for ease of reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dimension and output dimension for an MNIST classifier\n",
    "IN_DIM = 28 * 28\n",
    "OUT_DIM = 10\n",
    "# training settings\n",
    "LR = 10 ** -3\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCFF-NN Loss Landscapes on MNIST Classification Tasks\n",
    "We will be exploring the effect of a number of different architectural design choices on the loss landscapes of a fully connected feedforward neural network, in MNIST image classification tasks. We begin by defining a fully connected feedforward neural network in Pytorch, as well as a flattening transformation to be passed to the MNIST dataset loader. We also define a function for obtaining the model's loss in its current state. Note that this isn't an \"evaluation\" function where we'd want to use a test set - we're specifically going to evaluate on the train set, because we wish to visualize the loss landscape experienced by the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(IN_DIM, 512)\n",
    "        self.linear_2 = torch.nn.Linear(512, 256)\n",
    "        self.linear_3 = torch.nn.Linear(256, 128)\n",
    "        self.linear_4 = torch.nn.Linear(128, 64)\n",
    "        self.linear_5 = torch.nn.Linear(64, OUT_DIM)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        x = F.relu(self.linear_3(x))\n",
    "        x = F.relu(self.linear_4(x))\n",
    "        x = self.softmax(self.linear_5(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Flatten(object):\n",
    "    \"\"\" Transforms a PIL image to a flat numpy array. \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return np.array(sample, dtype=np.float32).flatten()\n",
    "    \n",
    "\n",
    "def evaluate(model):\n",
    "    mnist_train = datasets.MNIST(root='../data/', train=True, download=True, transform=Flatten())\n",
    "    trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    average_loss = 0\n",
    "    \n",
    "    # only evaluate on 10 batches for speed - ideally you'd want to evaluate on all data\n",
    "    for batch in itertools.islice(trainloader, 10):\n",
    "        x, y = batch\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        average_loss += loss\n",
    "    \n",
    "    average_loss /= 10\n",
    "    return average_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can carry out a few experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "In this first experiment we explore the effect of batch size on the loss landscape of our neural network when learning a straightforward MNIST classifier. To do so, we will train the model with a batch size of 1, and keep the model's initial and final parameters, and plot a linear interpolation between the two points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Batches:   0%|                                                                               | 0/60000 [00:00<?, ?it/s]\n",
      "Batches:   0%|                                                                       | 5/60000 [00:00<21:35, 46.30it/s]\n",
      "Batches:   0%|                                                                      | 10/60000 [00:00<21:53, 45.66it/s]\n",
      "Batches:   0%|                                                                      | 16/60000 [00:00<20:28, 48.83it/s]\n",
      "Batches:   0%|                                                                      | 26/60000 [00:00<17:26, 57.29it/s]\n",
      "Batches:   0%|                                                                      | 35/60000 [00:00<15:34, 64.16it/s]\n",
      "Batches:   0%|                                                                      | 45/60000 [00:00<14:13, 70.22it/s]\n",
      "Batches:   0%|                                                                      | 54/60000 [00:00<13:19, 74.99it/s]\n",
      "Batches:   0%|                                                                      | 63/60000 [00:00<12:51, 77.71it/s]\n",
      "Batches:   0%|                                                                      | 71/60000 [00:00<13:13, 75.50it/s]\n",
      "Batches:   0%|                                                                      | 79/60000 [00:01<13:09, 75.92it/s]\n",
      "Batches:   0%|                                                                      | 87/60000 [00:01<17:22, 57.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# download MNIST\n",
    "mnist_train = datasets.MNIST(root='../data/', train=True, download=True, transform=Flatten())\n",
    "trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=1, shuffle=True)\n",
    "\n",
    "# define model\n",
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# save initial state\n",
    "model_initial = copy.deepcopy(model)\n",
    "params_initial = copy.deepcopy(list(model_initial.parameters()))\n",
    "\n",
    "# train model\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    for count, batch in enumerate(tqdm(trainloader, 'Batches'), 0):\n",
    "        if count == 100:\n",
    "            break\n",
    "            \n",
    "        x, y = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# save final state\n",
    "model_final = copy.deepcopy(model)\n",
    "params_final = copy.deepcopy(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the `loss-landscapes` library to plot the loss along a line in parameter space from the initial parameters to the final parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-179b84305d07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_landscapes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_interpolation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Linear Interpolation of Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Parameter Space'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marce\\pycharmprojects\\loss-landscapes\\loss_landscapes\\compute.py\u001b[0m in \u001b[0;36mlinear_interpolation\u001b[1;34m(model_start, model_end, evaluation_f, steps)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_l2_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_displacement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_start_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_end_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0mdirection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_unit_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_displacement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_start_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_end_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;31m# compute and return losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marce\\pycharmprojects\\loss-landscapes\\loss_landscapes\\compute.py\u001b[0m in \u001b[0;36m_get_unit_vector\u001b[1;34m(vector, library)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_get_unit_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'torch'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss_landscapes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid library flag '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' passed to _get_unit_vector'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marce\\pycharmprojects\\loss-landscapes\\loss_landscapes\\backends\\torch\\ops.py\u001b[0m in \u001b[0;36munit_vector\u001b[1;34m(vector)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0munit_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mscalar_division\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marce\\pycharmprojects\\loss-landscapes\\loss_landscapes\\backends\\torch\\ops.py\u001b[0m in \u001b[0;36mscalar_division\u001b[1;34m(vector, scalar)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mscalar_division\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mscalar_multiplication\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "losses = loss_landscapes.compute.linear_interpolation(model_initial, model_final, evaluate)\n",
    "plt.plot(loss_data)\n",
    "plt.title('Linear Interpolation of Loss')\n",
    "plt.xlabel('Parameter Space')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear interpolation plot, as seen above, computes the model's loss at discrete intervals along a line between two points in parameter space. A common use for such a plot is computing the loss along the \"straight line path\" from the model's initialization to the model's final (trained) parameters. There is no guarantee that the path followed by the optimization procedure was \"close\" to this line. So what happens if we increase the batch size?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
