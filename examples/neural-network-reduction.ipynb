{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Neural Network Design Choices Affect Loss Landscapes\n",
    "In this notebook, we explore the appearance of the loss landscapes of neural networks on the MNIST image classification task, under a number of different transformations. We will be using the `loss-landscapes` package to compute low-dimensional approximations of the loss function. We will be implementing the networks in PyTorch, which is the only supported neural network library as of March 2019 (more will be added later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add project source to path for use in the notebook\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is required to add the module source to Python's `path` variable, so that this notebook can import the `loss-landscapes` package. It is not required in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [20, 8]\n",
    "\n",
    "# code from this library - import the lines module\n",
    "import loss_landscapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before we begin, we set some hyperparameters as constants for ease of reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dimension and output dimension for an MNIST classifier\n",
    "IN_DIM = 28 * 28\n",
    "OUT_DIM = 10\n",
    "# training settings\n",
    "LR = 10 ** -3\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCFF-NN Loss Landscapes on MNIST Classification Tasks\n",
    "We will be exploring the effect of a number of different architectural design choices on the loss landscapes of a fully connected feedforward neural network, in MNIST image classification tasks. We begin by defining a fully connected feedforward neural network in Pytorch, as well as a flattening transformation to be passed to the MNIST dataset loader. We also define a function for obtaining the model's loss in its current state. Note that this isn't an \"evaluation\" function where we'd want to use a test set - we're specifically going to evaluate on the train set, because we wish to visualize the loss landscape experienced by the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(IN_DIM, 512)\n",
    "        self.linear_2 = torch.nn.Linear(512, 256)\n",
    "        self.linear_3 = torch.nn.Linear(256, 128)\n",
    "        self.linear_4 = torch.nn.Linear(128, 64)\n",
    "        self.linear_5 = torch.nn.Linear(64, OUT_DIM)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        x = F.relu(self.linear_3(x))\n",
    "        x = F.relu(self.linear_4(x))\n",
    "        x = self.softmax(self.linear_5(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Flatten(object):\n",
    "    \"\"\" Transforms a PIL image to a flat numpy array. \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return np.array(sample, dtype=np.float32).flatten()    \n",
    "    \n",
    "\n",
    "def train(model, optimizer, criterion, train_loader, batches=100, epochs=1):\n",
    "    # save initial state\n",
    "    model_initial = copy.deepcopy(model)\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(epochs):\n",
    "        for count, batch in enumerate(train_loader, 0):\n",
    "            if count == batches:\n",
    "                break\n",
    "            \n",
    "            x, y = batch\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # save final state\n",
    "    model_final = copy.deepcopy(model)\n",
    "    \n",
    "    return model_initial, model_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can carry out a few experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "In this first experiment we explore the effect of batch size on the loss landscape of our neural network when learning a straightforward MNIST classifier. To do so, we will first train a classifier, in order to obtain two distinct points in the parameter space. Then, we will compute a set of values of the loss function in between these two points, first evaluating the loss with batches of size 8, then with batches of size 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_initial, model_final = train(MLP(), 100, 8, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the `loss-landscapes` library to plot the loss along a line in parameter space from the initial parameters to the final parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = loss_landscapes.compute.linear_interpolation(model_initial, model_final, make_evaluation(8))\n",
    "plt.plot(loss_data)\n",
    "plt.title('Linear Interpolation of Loss (Batch Size 8)')\n",
    "plt.xlabel('Parameter Space')\n",
    "plt.ylabel('Loss')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([2.2,2.4])\n",
    "plt.show()\n",
    "\n",
    "loss_data = loss_landscapes.compute.linear_interpolation(model_initial, model_final, make_evaluation(64))\n",
    "plt.plot(loss_data)\n",
    "plt.title('Linear Interpolation of Loss (Batch Size 64)')\n",
    "plt.xlabel('Parameter Space')\n",
    "plt.ylabel('Loss')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([2.2,2.4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear interpolation plot, as seen above, computes the model's loss at discrete intervals along a line between two points in parameter space. A common use for such a plot is computing the loss along the \"straight line path\" from the model's initialization to the model's final (trained) parameters, as we have done here. Observe that with larger batch sizes, the loss is smoother. This is because a larger batch suppresses the variations in the model's performance on individual samples. Note that there is no guarantee that the model followed this path during training.\n",
    "\n",
    "Next we will make some contour plots, both around the initial parameters and around the final parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = loss_landscapes.compute.random_plane(model_initial, make_evaluation(8), steps = 20)\n",
    "plt.contourf(loss_data)\n",
    "plt.title('Contour Plot of Loss Landscape (Batch Size 8)')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "X = np.array([[j for j in range(20)] for i in range(20)])\n",
    "Y = np.array([[i for _ in range(20)] for i in range(20)])\n",
    "ax.plot_surface(X, Y, loss_data, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Contour Plot of Loss Landscape (Batch Size 8)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since we're feeling fancy, let's make a surface plot as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = loss_landscapes.compute.random_plane(model_initial, make_evaluation(64), steps = 20)\n",
    "plt.contourf(loss_data)\n",
    "plt.title('Contour Plot of Loss Landscape (Batch Size 64)')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "X = np.array([[j for j in range(20)] for i in range(20)])\n",
    "Y = np.array([[i for _ in range(20)] for i in range(20)])\n",
    "ax.plot_surface(X, Y, loss_data, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Contour Plot of Loss Landscape (Batch Size 64)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = loss_landscapes.compute.random_plane(model_initial, make_evaluation(256), steps = 20)\n",
    "plt.contourf(loss_data)\n",
    "plt.title('Contour Plot of Loss Landscape (Batch Size 256)')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "X = np.array([[j for j in range(20)] for i in range(20)])\n",
    "Y = np.array([[i for _ in range(20)] for i in range(20)])\n",
    "ax.plot_surface(X, Y, loss_data, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Contour Plot of Loss Landscape (Batch Size 256)');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
