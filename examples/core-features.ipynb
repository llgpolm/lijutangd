{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Features: Lines and Planes in Parameter Space\n",
    "\n",
    "This example covers the basic features of the `loss-landscapes` library, i.e. evaluating a model's loss function along lines or planes in parameter space in order to produce visualizations of the loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [18, 12]\n",
    "\n",
    "# code from this library - import the lines module\n",
    "import loss_landscapes\n",
    "import loss_landscapes.utils\n",
    "import loss_landscapes.evaluators.torch as evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminary: Classifying MNIST\n",
    "\n",
    "This notebook demonstrates how to accomplish a simple task: visualizing the loss landscape of a small fully connected feed-forward neural network on the MNIST image classification task. In this section the preliminaries (the model and the training procedure) are setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "IN_DIM = 28 * 28\n",
    "OUT_DIM = 10\n",
    "LR = 10 ** -2\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 25\n",
    "# contour plot resolution\n",
    "STEPS = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells in this section contain no code specific to the `loss-landscapes` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPSmall(torch.nn.Module):\n",
    "    \"\"\" Fully connected feed-forward neural network with one hidden layer. \"\"\"\n",
    "    def __init__(self, x_dim, y_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(x_dim, 32)\n",
    "        self.linear_2 = torch.nn.Linear(32, y_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.linear_1(x))\n",
    "        return F.softmax(self.linear_2(h), dim=1)\n",
    "\n",
    "\n",
    "class Flatten(object):\n",
    "    \"\"\" Transforms a PIL image to a flat numpy array. \"\"\"\n",
    "    def __call__(self, sample):\n",
    "        return np.array(sample, dtype=np.float32).flatten()    \n",
    "    \n",
    "\n",
    "def train(model, optimizer, criterion, train_loader, epochs):\n",
    "    \"\"\" Trains the given model with the given optimizer, loss function, etc. \"\"\"\n",
    "    model.train()\n",
    "    # train model\n",
    "    for _ in tqdm(range(epochs), 'Training'):\n",
    "        for count, batch in enumerate(train_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            x, y = batch\n",
    "\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then trained, producing a start point and an end point for plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download MNIST and setup data loaders\n",
    "mnist_train = datasets.MNIST(root='../data', train=True, download=True, transform=Flatten())\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# define model and deepcopy initial model\n",
    "model = MLPSmall(IN_DIM, OUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Points in Parameter Space\n",
    "The state of a neural network model represents a point in parameter space. Several functions in the `loss-landscapes` library require the user to provide instances of neural network models as arguments, where the model is taken to represent the point in parameter space corresponding to its parameters' state. Representing points as simply the models they relate to reduces the complexity of the client code.\n",
    "\n",
    "In the cell below, a copy of the model before training is made, to preserve the model's initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the initial point in parameter space\n",
    "model_initial = loss_landscapes.utils.deepcopy_model(model, 'torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two common points of interest are the model initialization, and the model's final parameters after training. Similarly to the cell above, we can make a copy of the model after training. A copy of the model can be made at any time - this is up to the user, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, criterion, train_loader, EPOCHS)\n",
    "\n",
    "model_final = loss_landscapes.utils.deepcopy_model(model, 'torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Interpolations of Loss between Two Points\n",
    "\n",
    "In the previous section, copies of the model before and after training were obtained. A common use case is to evaluate the model loss at a number of equidistant points along the straight line connecting the two points. Although care must be taken in interpreting such plots, the idea is to gain an insight into the smoothness of the landscape.\n",
    "\n",
    "The user might also be interested in collecting any other quantity along this line. The `loss-landscapes` library abstracts these details with entities called `Evaluator`s, which compute some quantity about a model for its current parameters. A number of pre-defined evaluators is provided in the `loss_landscapes.evaluators` package, and the user is free to write custom evaluators. See the relevant documentation.\n",
    "\n",
    "An important evaluator for use with PyTorch models is the torch `LossEvaluator`, which applies a PyTorch loss function to a PyTorch model and returns the value produced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data that the evaluator will use when evaluating loss\n",
    "x, y = iter(train_loader).__next__()\n",
    "loss_evaluator = evaluators.LossEvaluator(criterion, x, y)\n",
    "\n",
    "# compute loss data\n",
    "loss_data = loss_landscapes.linear_interpolation(model_initial, model_final, loss_evaluator, STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the computed loss data, a linear interpolation plot can be rendered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1/STEPS * i for i in range(STEPS)], loss_data)\n",
    "plt.title('Linear Interpolation of Loss')\n",
    "plt.xlabel('Interpolation Coefficient')\n",
    "plt.ylabel('Loss')\n",
    "axes = plt.gca()\n",
    "# axes.set_ylim([2.300,2.325])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Planar Approximations of Loss Around a Point\n",
    "\n",
    "Another core use for the library is producing 2-dimensional approximations of the loss-landscape topology around a point in parameter space. This is accomplished by sampling two random direction vectors in parameter space, and computing the loss at a number of points on the plane defined by the two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data_fin = loss_landscapes.random_plane(model_final, loss_evaluator, 10, STEPS, 'layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss values on this plane can be visualized in an intuitive and interpretable manner using contour plots or 3D surface plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(loss_data_fin, levels=50)\n",
    "plt.title('Loss Contours around Trained Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "X = np.array([[j for j in range(STEPS)] for i in range(STEPS)])\n",
    "Y = np.array([[i for _ in range(STEPS)] for i in range(STEPS)])\n",
    "ax.plot_surface(X, Y, loss_data_fin, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Surface Plot of Loss Landscape')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
