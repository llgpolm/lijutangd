{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add project source to path for use in the notebook\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "project_dir = '\\\\'.join(list(os.getcwd().split('\\\\')[:-1]))\n",
    "\n",
    "sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing some required libraries, including the `loss_landscapes.lines` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [20, 8]\n",
    "\n",
    "# code from this library - import the lines module\n",
    "import loss_landscapes.lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Interpolation without Filter Normalization\n",
    "The first use case we demonstrate is producing a linear interpolation plot (without filter normalization). We begin by defining a neural network model using `pytorch`. The model is going to be a convnet, which we will train (only a little) on MNIST before producing a linear interpolation plot of the loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = torch.nn.Conv2d(1, 25, kernel_size=(11,11), stride=(1,1))  # produces 25 16x16 feature maps\n",
    "        self.conv_2 = torch.nn.Conv2d(25, 64, kernel_size=(9,9), stride=(1,1))   # produces 64 10x10 feature maps\n",
    "        self.maxpool_1 = torch.nn.MaxPool2d(2,2)                                 # produces 64 5x5 feature maps\n",
    "        self.linear_1 = torch.nn.Linear(64 * 5 * 5, 400)\n",
    "        self.linear_2 = torch.nn.Linear(400, 10)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_1(x))          # conv 1\n",
    "        x = F.relu(self.conv_2(x))          # conv 2\n",
    "        x = self.maxpool_1(x)               # pooling\n",
    "        x = x.view(-1, 64 * 5 * 5)          # flatten\n",
    "        x = F.relu(self.linear_1(x))        # dense 1\n",
    "        x = self.softmax(self.linear_2(x))  # dense 2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we write a standard training loop, with the exception of holding on to the model's initial `state_dict` and its final `state_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download MNIST\n",
    "mnist_train = datasets.MNIST(root='../data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=1, shuffle=True)\n",
    "\n",
    "# define model\n",
    "model = Network()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# save initial state_dict\n",
    "start_params = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# train model\n",
    "for epoch in range(1):\n",
    "    print('Epoch ' + str(epoch) + ':')\n",
    "    for batch in tqdm(trainloader, 'MNIST:'):\n",
    "        x, y = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "end_params = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define an evaluation function, which the `loss_landscapes` library will use to compute the loss for a particular set of parameters. Note that the function below does not use the entire dataset to compute the loss, but rather only uses a random sample of 128 images split into 4 batches of 32. This is just to allow the notebook to terminate sooner. Note that this makes the linear interpolation plot essentially meaningless - if you wish to try this \"for real\", uncomment the commented out code in the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    mnist_train = datasets.MNIST(root='../data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "    trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    average_loss = 0 \n",
    "    for batch in itertools.islice(trainloader, 4):\n",
    "        x, y = batch\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        average_loss += loss\n",
    "    \n",
    "    average_loss /= 4\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, below is a more meaningful, but more computationally intensive evaluation function. If you wish to use this one to generate the linear interpolation, edit the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_for_real(model):\n",
    "    mnist_train = datasets.MNIST(root='../data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "    trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    average_loss = 0 \n",
    "    for batch in trainloader:\n",
    "        x, y = batch\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        average_loss += loss\n",
    "    \n",
    "    average_loss /= 1875\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use this to compute a linear interpolation, which we then visualize as a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want a proper evaluation, uncomment this line and comment out the one below\n",
    "# loss_data = loss_landscapes.lines.linear_interpolation(model, start_params, end_params, evaluate_for_real, 100)\n",
    "\n",
    "loss_data = loss_landscapes.lines.linear_interpolation(model, start_params, end_params, evaluate, 100)\n",
    "plt.plot(loss_data)\n",
    "plt.title('Linear Interpolation of Loss')\n",
    "plt.xlabel('Parameter Space')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
